{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🗞️ News Classifier\n",
    "### - ___Building an Automated News Classification System with NLP Techniques___.\n",
    "-------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Extracting the news headlines url of different news categories\n",
    "- ### **\"selenium**\" is used to scrape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary Libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting with MYSQL database  to pull and store the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the mysql library to connect mysql database with python script\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting with MYSQL database\n",
    "try:\n",
    "    my_db = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"123\",\n",
    "        database=\"news_classifier\" \n",
    "    )\n",
    "    print(\"Connected to MySQL database successfully.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(\"Error connecting to MySQL:\", err)\n",
    "#cursor object\n",
    "my_cursor=my_db.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the database to retrieve the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_query = \"SELECT * FROM category_url\"\n",
    "my_cursor.execute(select_query)\n",
    "#-----------------------------------------------------------------------------\n",
    "#lists to store the data\n",
    "category_list=[]\n",
    "url_list=[]\n",
    "for row in my_cursor.fetchall():\n",
    "     category_list.append(row[1])\n",
    "     url_list.append(row[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the data into  Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary of data\n",
    "data={\"category\":category_list,\"url\":url_list}\n",
    "#pandas dataframe\n",
    "category_url_df=pd.DataFrame(data)\n",
    "category_url_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ###  Note: Each category **\"2000\"** different **news headlines** url's are extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business,Education and Sports Topics URL Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_headlines_url_1(category_link,news_count):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {\n",
    "            \"profile.managed_default_content_settings.images\": 2,\n",
    "            \"profile.managed_default_content_settings.videos\": 2,\n",
    "            \"profile.managed_default_content_settings.gifs\": 2\n",
    "        }\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        try:\n",
    "            driver.set_page_load_timeout(15)\n",
    "            driver.get(category_link);\n",
    "        except:\n",
    "              pass\n",
    "        time.sleep(3) \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")       \n",
    "        #-----------------------------------------------------------------------------------------------------\n",
    "        elements_per_page=25\n",
    "        total_pages=math.ceil(news_count/elements_per_page)\n",
    "        headlines_url_list=[]\n",
    "        for page in tqdm(range(total_pages), desc=\"Processing\", unit=\"iteration\"):\n",
    "                try:\n",
    "                    elements=driver.find_elements(By.CSS_SELECTOR,\"div.img-context>h2>a\") #Using CSS_SELECTOR Finding the Required Web element\n",
    "                    try:\n",
    "                        for element in elements:\n",
    "                            if len(headlines_url_list) >=news_count: #Breaks the loop if the list reaches needed quantity \n",
    "                                break\n",
    "                            headline_url=element.get_attribute(\"href\") #Getting hyperlink from the web element\n",
    "                            headlines_url_list.append(headline_url)\n",
    "                    except:\n",
    "                            headlines_url_list.append(None)\n",
    "                    next_button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//a[@class=\"next page-numbers\"]'))) #Using XPATH Finding the \"Next\" Button\n",
    "                    next_button.click()  \n",
    "                    \n",
    "                except Exception as e:\n",
    "                      print(e)\n",
    "                      pass\n",
    "        return headlines_url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function to extract the data\n",
    "business_headlines_url=news_headlines_url_1(category_url_df.iloc[0,1],2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Migrate the data into MYSQL database\n",
    "try:  \n",
    "        #Create  table \n",
    "        create_table_query = \"CREATE TABLE IF NOT EXISTS business_headlines_url(id INT AUTO_INCREMENT PRIMARY KEY,url TEXT)\"                             \n",
    "        my_cursor.execute(create_table_query)\n",
    "        my_db.commit()\n",
    "        print('\"business_headlines_url\" table created successfully.')\n",
    "except Exception as e :\n",
    "       print(e)\n",
    "for url in business_headlines_url:\n",
    "        insert_query = \" INSERT INTO business_headlines_url(url) VALUES (%s) \"\n",
    "        values = (url,)\n",
    "        my_cursor.execute(insert_query, values)\n",
    "        my_db.commit()\n",
    "print(\"Data Migrated Sucessfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function to extract the data\n",
    "sports_headlines_url=news_headlines_url_1(category_url_df.iloc[2,1],2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Migrate the data into MYSQL database\n",
    "try:  \n",
    "        #Create  table \n",
    "        create_table_query = \"CREATE TABLE IF NOT EXISTS sports_headlines_url(id INT AUTO_INCREMENT PRIMARY KEY,url TEXT)\"                             \n",
    "        my_cursor.execute(create_table_query)\n",
    "        my_db.commit()\n",
    "        print('\"sports_headlines_url\" table created successfully.')\n",
    "except Exception as e :\n",
    "       print(e)\n",
    "for url in sports_headlines_url:\n",
    "        insert_query = \" INSERT INTO sports_headlines_url(url) VALUES (%s) \"\n",
    "        values = (url,)\n",
    "        my_cursor.execute(insert_query, values)\n",
    "        my_db.commit()\n",
    "print(\"Data Migrated Sucessfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Education "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function to extract the data\n",
    "education_headlines_url=news_headlines_url_1(category_url_df.iloc[1,1],60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Migrate the data into MYSQL database\n",
    "try:  \n",
    "        #Create  table \n",
    "        create_table_query = \"CREATE TABLE IF NOT EXISTS education_headlines_url(id INT AUTO_INCREMENT PRIMARY KEY,url TEXT)\"                             \n",
    "        my_cursor.execute(create_table_query)\n",
    "        my_db.commit()\n",
    "        print('\"education_headlines_url\" table created successfully.')\n",
    "except Exception as e :\n",
    "       print(e)\n",
    "for url in education_headlines_url:\n",
    "        insert_query = \" INSERT INTO education_headlines_url(url) VALUES (%s) \"\n",
    "        values = (url,)\n",
    "        my_cursor.execute(insert_query, values)\n",
    "        my_db.commit()\n",
    "print(\"Data Migrated Sucessfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entertainment Topics URL Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_headlines_url_2(category_link,news_count):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {\n",
    "            \"profile.managed_default_content_settings.images\": 2,\n",
    "            \"profile.managed_default_content_settings.videos\": 2,\n",
    "            \"profile.managed_default_content_settings.gifs\": 2\n",
    "        }\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        try:\n",
    "            driver.set_page_load_timeout(15)\n",
    "            driver.get(category_link);\n",
    "        except:\n",
    "              pass\n",
    "        time.sleep(3) \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")       \n",
    "        #-----------------------------------------------------------------------------------------------------\n",
    "        elements_per_page=25\n",
    "        total_pages=math.ceil(news_count/elements_per_page)\n",
    "        headlines_url_list=[]\n",
    "        for page in tqdm(range(total_pages), desc=\"Processing\", unit=\"iteration\"):\n",
    "                try:\n",
    "                    elements=driver.find_elements(By.CSS_SELECTOR,\"div.img-context > div.title >a\") #Using CSS_SELECTOR Finding the Required Web element\n",
    "                    try:\n",
    "                        for element in elements:\n",
    "                            if len( headlines_url_list) >=news_count: #Breaks the loop if the list reaches needed quantity \n",
    "                                break\n",
    "                            headline_url=element.get_attribute(\"href\") #Getting hyperlink from the web element\n",
    "                            headlines_url_list.append(headline_url)\n",
    "                    except:\n",
    "                             headlines_url_list.append(None)\n",
    "                    next_button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//a[@class=\"next page-numbers\"]'))) #Using XPATH Finding the \"Next\" Button\n",
    "                    next_button.click()  \n",
    "                    \n",
    "                except Exception as e:\n",
    "                      print(e)\n",
    "                      pass\n",
    "        return headlines_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function to extract the data\n",
    "entertainment_headlines_url=news_headlines_url_2(category_url_df.iloc[4,1],2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Migrate the data into MYSQL database\n",
    "try:  \n",
    "        #Create  table \n",
    "        create_table_query = \"CREATE TABLE IF NOT EXISTS entertainment_headlines_url(id INT AUTO_INCREMENT PRIMARY KEY,url TEXT)\"                             \n",
    "        my_cursor.execute(create_table_query)\n",
    "        my_db.commit()\n",
    "        print('\"entertainment_url\" table created successfully.')\n",
    "except Exception as e :\n",
    "       print(e)\n",
    "for url in entertainment_headlines_url:\n",
    "        insert_query = \" INSERT INTO entertainment_headlines_url(url) VALUES (%s) \"\n",
    "        values = (url,)\n",
    "        my_cursor.execute(insert_query, values)\n",
    "        my_db.commit()\n",
    "print(\"Data Migrated Sucessfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technology Topic URL Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_headlines_url_3(category_link,news_count):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {\n",
    "            \"profile.managed_default_content_settings.images\": 2,\n",
    "            \"profile.managed_default_content_settings.videos\": 2,\n",
    "            \"profile.managed_default_content_settings.gifs\": 2\n",
    "        }\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        try:\n",
    "            driver.set_page_load_timeout(15)\n",
    "            driver.get(category_link);\n",
    "        except:\n",
    "              pass\n",
    "        time.sleep(3) \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")       \n",
    "        #-----------------------------------------------------------------------------------------------------\n",
    "        elements_per_page=20\n",
    "        total_pages=math.ceil(news_count/elements_per_page)\n",
    "        headlines_url_list=[]\n",
    "        for page in tqdm(range(total_pages), desc=\"Processing\", unit=\"iteration\"):\n",
    "                try:\n",
    "                    elements=driver.find_elements(By.CSS_SELECTOR,\"div.top-article > ul > li h3 > a\") #Using CSS_SELECTOR Finding the Required Web element\n",
    "                    try:\n",
    "                        for element in elements:\n",
    "                            if len( headlines_url_list) >=news_count: #Breaks the loop if the list reaches needed quantity \n",
    "                                break\n",
    "                            headline_url=element.get_attribute(\"href\") #Getting hyperlink from the web element\n",
    "                            headlines_url_list.append(headline_url)\n",
    "                    except:\n",
    "                             headlines_url_list.append(None)\n",
    "                    next_button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//a[@class=\"next page-numbers\"]'))) #Using XPATH Finding the \"Next\" Button\n",
    "                    next_button.click()  \n",
    "                    \n",
    "                except Exception as e:\n",
    "                      print(e)\n",
    "                      pass\n",
    "        return headlines_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function to extract the data\n",
    "technology_headlines_url=news_headlines_url_3(category_url_df.iloc[3,1],2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Migrate the data into MYSQL database\n",
    "try:  \n",
    "        #Create  table \n",
    "        create_table_query = \"CREATE TABLE IF NOT EXISTS technology_headlines_url(id INT AUTO_INCREMENT PRIMARY KEY,url TEXT)\"                             \n",
    "        my_cursor.execute(create_table_query)\n",
    "        my_db.commit()\n",
    "        print('\"technology_url\" table created successfully.')\n",
    "except Exception as e :\n",
    "       print(e)\n",
    "for url in technology_headlines_url:\n",
    "        insert_query = \" INSERT INTO technology_headlines_url(url) VALUES (%s) \"\n",
    "        values = (url,)\n",
    "        my_cursor.execute(insert_query, values)\n",
    "        my_db.commit()\n",
    "print(\"Data Migrated Sucessfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
