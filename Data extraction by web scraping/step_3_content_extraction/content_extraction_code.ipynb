{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ—žï¸ News Classifier\n",
    "### - ___Building an Automated News Classification System with NLP Techniques___.\n",
    "-------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 :  Extracting News Contents using Headlines url, by web scraping .\n",
    " - ### **\"Selenium\"** is used for scraping data from the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary Libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException,NoSuchWindowException\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection with MYSQL Database to pull and store the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the mysql library to connect mysql database with python script\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL database successfully.\n"
     ]
    }
   ],
   "source": [
    "#Connecting with MYSQL database\n",
    "try:\n",
    "    my_db = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"123\",\n",
    "        database=\"news_classifier\")\n",
    "    print(\"Connected to MySQL database successfully.\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(\"Error connecting to MySQL:\", err)\n",
    "#cursor object\n",
    "my_cursor=my_db.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ### Loading the Headlines url data from MYSQL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_table_list=[\"business_headlines_url\",\"education_headlines_url\",\"sports_headlines_url\",\"technology_headlines_url\",\"entertainment_headlines_url\"]\n",
    "#lists to store the urls \n",
    "business_url_list=[]\n",
    "education_url_list=[]\n",
    "sports_url_list=[]\n",
    "technology_url_list=[]\n",
    "entertainment_url_list=[]\n",
    "#--------------------------------------------------------------------------------------\n",
    "#Querying the database to retrieve the required data\n",
    "for table in category_table_list:\n",
    "          select_query = f\"SELECT * FROM {table}\"\n",
    "          my_cursor.execute(select_query)\n",
    "          #-----------------------------------------------------------------------------\n",
    "          #lists to store the data\n",
    "          for row in my_cursor.fetchall():\n",
    "                    if table==\"business_headlines_url\":\n",
    "                         business_url_list.append(row[1])\n",
    "                    if table==\"education_headlines_url\":\n",
    "                         education_url_list.append(row[1])\n",
    "                    if table==\"sports_headlines_url\":\n",
    "                         sports_url_list.append(row[1])\n",
    "                    if table==\"technology_headlines_url\":\n",
    "                         technology_url_list.append(row[1])\n",
    "                    if table==\"entertainment_headlines_url\":\n",
    "                         entertainment_url_list.append(row[1])\n",
    "          break\n",
    "#----------------------------------------------------------------------------------------\n",
    "#Creating dataframes of induvidual categories \n",
    "               \n",
    "#business \n",
    "business_headlines_url_df=pd.DataFrame(business_url_list,columns=[\"url\"])\n",
    "#education\n",
    "education_headlines_url_df=pd.DataFrame(education_url_list,columns=[\"url\"])\n",
    "#sports\n",
    "sports_headlines_url_df=pd.DataFrame(sports_url_list,columns=[\"url\"])\n",
    "#technology\n",
    "technology_headlines_url_df=pd.DataFrame(technology_url_list,columns=[\"url\"])\n",
    "#entertainment\n",
    "entertainment_headlines_url_df=pd.DataFrame(entertainment_url_list,columns=[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating table in \"news_classifier\" database to store the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------\n",
    "#Creating a table to store the data\n",
    "create_table_query = \"\"\"CREATE TABLE IF NOT EXISTS content(\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        headline TEXT,\n",
    "        description TEXT,\n",
    "        content TEXT,\n",
    "        url TEXT,\n",
    "        category VARCHAR(50)\n",
    "    )\n",
    "    \"\"\"\n",
    "my_cursor.execute(create_table_query)\n",
    "my_db.commit()\n",
    "print('\"content\" table successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction by web scraping :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists to hold the respective scraped data\n",
    "headlines_list=[]\n",
    "description_list=[]\n",
    "content_list=[]\n",
    "url_list=[]\n",
    "category_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for extracting the content from headlines url\n",
    "def content_extraction(headlines_df,starting_index,ending_index,category):\n",
    "            #-------------------------------------------------------------------------------\n",
    "            #Using selenium;s google chrome web driver\n",
    "            chrome_options = webdriver.ChromeOptions()\n",
    "            #Turn off the images,gifs,videos to avoid increased loading time, because these informations are not needed\n",
    "            prefs = {\n",
    "                    \"profile.managed_default_content_settings.images\": 2,\n",
    "                    \"profile.managed_default_content_settings.gifs\": 2,\n",
    "                    \"profile.managed_default_content_settings.videos\": 2\n",
    "                    }\n",
    "            chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "            #------------------------------------------------------------------------------\n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            print(starting_index,\"--\",ending_index)\n",
    "            current_index=starting_index\n",
    "            for index, url in tqdm(headlines_df.iloc[starting_index:ending_index, :].iterrows(), desc=\"Processing\", total=(ending_index-starting_index)):\n",
    "                    #Another function for harvesting the data, this function handles exceptions very well and helps in extracting the data efficintly.\n",
    "                    def harvest_data():\n",
    "                                nonlocal current_index,category\n",
    "                                try:\n",
    "                                    driver.set_page_load_timeout(12)\n",
    "                                    driver.get(url.iloc[0]);\n",
    "                                    time.sleep(3)\n",
    "                                    try : \n",
    "                                            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                                            content=driver.find_element(By.CSS_SELECTOR,\"#pcl-full-content \")\n",
    "                                            content_list.append(content.text)\n",
    "                                            headlines = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div:nth-child(1) > div > h1\"))) \n",
    "                                            headlines_list.append(headlines.text)\n",
    "                                            description=driver.find_element(By.CSS_SELECTOR,\"div:nth-child(1) > div > h2\")\n",
    "                                            description_list.append(description.text)\n",
    "                                            url_list.append(url.iloc[0])\n",
    "                                            category_list.append(category)\n",
    "                                            #----------------------------------------\n",
    "                                            print(\"Iterated_Upto :\",current_index)\n",
    "                                            current_index=int(current_index)+1\n",
    "                                    except (TimeoutError,TimeoutException):\n",
    "                                             print(\"Page Reloaded\")\n",
    "                                             harvest_data() \n",
    "                                             pass\n",
    "                                    except (NoSuchElementException): \n",
    "                                            try:\n",
    "                                                        nocontent=driver.find_element(By.CSS_SELECTOR,\"  body > p\")\n",
    "                                                        headlines_list.append(None) \n",
    "                                                        description_list.append(None)\n",
    "                                                        content_list.append(None)\n",
    "                                                        url_list.append(url.iloc[0])\n",
    "                                                        category_list.append(category)\n",
    "                                                        print(\"no_content,Iterated_Upto :\",current_index)\n",
    "                                            except Exception:\n",
    "                                                        print(\"Iterated_Upto :\",current_index)\n",
    "                                                        current_index=int(current_index)\n",
    "                                                        pass\n",
    "                                except (TimeoutException) :\n",
    "                                    print(\"Exception,reloading...\",)\n",
    "                                    harvest_data() \n",
    "                    harvest_data() \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to upload the data into MYSQL database\n",
    "def upload_to_mysql():\n",
    "            for i in range(len(headlines_list)):\n",
    "                        insert_query = \"INSERT INTO content(headline, description, content, url, category) VALUES (%s, %s, %s, %s, %s) \"\n",
    "                        values = (headlines_list[i], description_list[i], content_list[i], url_list[i], category_list[i])\n",
    "                        my_cursor.execute(insert_query,values)\n",
    "                        my_db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of dataframes\n",
    "df_list=[business_headlines_url_df,education_headlines_url_df,sports_headlines_url_df,technology_headlines_url_df,entertainment_headlines_url_df]\n",
    "category_name_list=[\"bussiness\",\"education\",\"sports\",\"technology\",\"entertainment\"]\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "for df ,category in zip(df_list,category_name_list):\n",
    "            headlines_df   =df\n",
    "            starting_index =0\n",
    "            ending_index   =len(df)\n",
    "            #---------------------------------------------------------------------------\n",
    "            #Calling the function to extract the content by headlines url\n",
    "            content_extraction(headlines_df,starting_index,ending_index,category)\n",
    "            #---------------------------------------------------------------------------\n",
    "            #Calling the function to store the Extrcated information in MYSQL Database.\n",
    "            upload_to_mysql()\n",
    "            #---------------------------------------------------------------------------\n",
    "            #Remove the elements the list after uploading \n",
    "            headlines_list.clear()\n",
    "            description_list.clear()\n",
    "            url_list.clear()\n",
    "            category_list.clear()\n",
    "            print(category, \"data stored to database sucessfully.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
